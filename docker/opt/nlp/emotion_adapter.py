"""
ãƒãƒ³ãƒ€ã‚¤ãƒŠãƒ ã‚³ç ”ç©¶æ‰€ æä¾› æ„Ÿæƒ…åˆ¤å®šAdapters
"""
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdapterType
from numpy import average
import pickle as pick

MODEL_DIR = "./nlp/"
def setup_model():
    """
    èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘å®Ÿè¡Œã•ã‚Œã‚‹
    æä¾›ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’DLã—ã¦pickleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ä¿å­˜ã™ã‚‹
    """
    model = AutoModelForSequenceClassification.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
    tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
    
    with open(MODEL_DIR + 'model.pickle', 'wb') as f:
        pick.dump(model, f)
    with open(MODEL_DIR + 'tokenizer.pickle', 'wb') as f:
        pick.dump(tokenizer, f)

def load_model():
    """
    ä¿å­˜ã—ãŸæä¾›ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€

    Returns
    ----------
    model
        æä¾›ã•ã‚ŒãŸmodel
    tokernizre
        æä¾›ã•ã‚ŒãŸãƒ¤ãƒ„(ã‚ˆãã‚ã‹ã£ã¦ãªã„)
    """
    with open(MODEL_DIR + 'model.pickle', 'rb') as f:
        model = pick.load(f)
    with open(MODEL_DIR + 'tokenizer.pickle', 'rb') as f:
        tokenizer = pick.load(f)
    return model, tokenizer

def tweets2posi_nega(tweets):
    """
    ãƒ„ã‚¤ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆã‹ã‚‰ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã®åˆ¤å®šã‚’ã™ã‚‹.
    å…¨ãƒ„ã‚¤ãƒ¼ãƒˆã®å¹³å‡ã‚’è¿”ã™0ã‹ã‚‰1ã§è¿”ã™. 1ã«è¿‘ã„ã»ã©ãƒã‚¸ãƒ†ã‚£ãƒ–.

    Parameters
    ----------
    tweets : [String]
        ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã®ãƒªã‚¹ãƒˆ

    Return
    ----------
    float
        1ã«è¿‘ã„: ãƒã‚¸ãƒ†ã‚£ãƒ–
        0ã«è¿‘ã„: ãƒã‚¬ãƒ†ã‚£ãƒ–
    """
    # ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    model, tokenizer = load_model()

    results = []
    for tweet in tweets:
        if tweet == "":
            continue
        token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet))
        input_tensor = torch.tensor([token_ids])
        outputs = model(input_tensor, adapter_names=['sst-2'])
        result = torch.argmax(outputs[0]).item()
        results.append(result)
        # print(f"{result}: {tweet}") # ç¢ºèªç”¨
    del model
    del tokenizer
    return average(results)

def predict(sentence, tokenizer, model):
    """
    1ã¤ã®æ–‡ç« ã®ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’åˆ¤å®šã™ã‚‹.
    
    Parameters
    ----------
    sentence : String
        è§£æã—ãŸã„æ–‡ç« 
    tokenizer : transformers.models.bert.modeling_bert.BertForSequenceClassification
    model : transformers.models.bert_japanese.tokenization_bert_japanese.BertJapaneseTokenizer

    Return
    ----------
    int : åˆ¤å®šçµæœ
        1: ãƒã‚¸ãƒ†ã‚£ãƒ–
        0: ãƒã‚¬ãƒ†ã‚£ãƒ–
    """
    
    token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))
    input_tensor = torch.tensor([token_ids])
    outputs = model(input_tensor, adapter_names=['sst-2'])
    result = torch.argmax(outputs[0]).item()

    return result

if __name__ == "__main__":
    
    # setup_model()

    # ãƒ†ã‚¹ãƒˆç”¨
    print(tweets2posi_nega([
        "ã‚¦ãƒã‚‰ã‚ºãƒƒå‹ã ãƒ§",
        "æ¥½ã—ã„",
        "å…¨ç„¶é¢ç™½ããªã„",
        "ã†ã¡ï¼ˆãƒ¦ãƒ‹ãƒƒãƒˆï¼‰ã®ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰ä¸Šã’ã‚‰ã‚Œã‚‹ã®ã€M3å‰æ—¥ã®å¤œã˜ã‚ƒãªã„ã‹ãªğŸ˜‡",
        "ã“ã‚“ãªãˆã’ã¤ãªã„å¤§å­¦é€²å­¦ç‡ã§å¤§å­¦ã®ç„¡å„ŸåŒ–ãªã‚“ã„ç„¡æ„å‘³ã ã¹",
        "ã‚ãƒ¼ã‚“ã€ç´ æ•µâ€¦ï¼",
        "æ˜¥ã®M3ã¯ã­â€¦è¡ŒããŸã„ã‚ã­â€¦",
        "M3ã‚’ä¹—ã‚Šåˆ‡ã‚‹ä½“åŠ›ãŒã‚ã‚‹ã®ã‹å¿ƒé…ã ãâ€¦â€¦ã€‚",
    ]))
